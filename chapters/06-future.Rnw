There are many possible extensions and future projects to the thesis at hand.
Estimation uncertainty is the primary motivation for this thesis.
Bayesian statistics provide a straightforward way to integrate that. 
However, it demands indepth knowledge of Markov Chain Monte Carlo and also how to construct good prior distributions.
Neither are easy tasks.
Another approach of incorporating estimation uncertainty is robust optimization.
Robust optimization, in a MPT setting, tries to incorporate the estimation uncertainty into the portfolio allocation problem itself.
Are there connections to be made and especially with empirical bayes?

Paper \hyperref[sec:paper3]{3} assumes that the rebalancing points are fixed.
This assumption can be limiting for some investors.
Can those be exchanged for stopping times, incorporated in the decision process and the portfolio allocation problem?

Many Multivariate GARCH models can be formulated as the following BEKK model (see, e.g., \citet{engle1995multivariate})
\begin{equation}\label{eqn:BEKK}
  \bH_t = \bC \bC^\top + \sum_{k=1}^K \bA_k \boldsymbol{\epsilon}_{t-1}\boldsymbol{\epsilon}_{t-1}^\top \bA_k^\top + \sum_{k=1}^K \bG_k \bH_{t-1}\bG_k^\top,
\end{equation}
where $\bH_i$ is a sequence of conditional covariance matrices, $\boldsymbol{\epsilon}_t | \mathcal{F}_{t-1} \sim N_p(\mathbf{0], \bH_t})$ and the matrices $\bC, \bA_i$ and $\bG_i$ are of appropriate dimensions.
These are usually very hard to fit and use for portfolio allocations. 
The first issue is due to the number of parameters in the model.
There are a number of parametrisations but if all matrices are symmetric then there are $(K+1/2)p(p+1)$ parameters to estimate. 
Building a portfolio of size 10 with $K=1$ implies that $165$ parameters need to be estimated.
Furthermore, although the constraints should enforce forecasts which are positive definite it is not necessarily true that they will be numerically invertible.
It can provide forecasts which are very close to singular.
The first issue can be solved if one can formulate the models as Recurrent Neural Networks and use deep-learning libraries Torch or Tensorflow to fit the models. 
These are tailored to solve the problem of fitting very large models.
Recent large Natural Language Processing models have \textit{billions} of parameters (see, e.g., \citet{brown2020language}). 
The second problem can then possibly be solved by placing BEKK models in this framework.
Positive definite forecasts could then be enforced by developing new layers to the network. 
Furthermore, it would also be easier to integrate different sources of information in the models in the context of these models.
