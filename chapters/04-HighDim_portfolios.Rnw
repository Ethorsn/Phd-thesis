In the previous chapter we presented different ways of estimating the covariance matrix. Under certain conditions and/or statistical models, the sample covariance matrix inherited certain properties. 
%If we hold $p$ constant and let $n$ grow the sample covariance matrix is consistent. 
If we have a lot of data on the assets that we are trying to invest in then we can most often be certain that we will hold the correct portfolio.
Our estimated portfolio will be consistent, e.g. it estimates the correct object of interest. 
However, if we believe in diversification then $p$ should be big as well. 
It should, in theory, decrease the risk (variance) of the portfolio. 
That is not always the case.
By introducing one new asset to our portfolio of size $p$ we need to estimate all covariances for that asset in the sample covariance matrix. They will constitute an additional $p+1$ quantities. 
The sample covariance matrix suffers from the curse of dimensionality. 
In terms of estimation uncertainty, this does not scale well.
From \citet{bodnar2016optimal} Proposition 2.2 we know that $\hV \rightarrow \V/(1-c)$ whenever $p,n \rightarrow \infty$ s.t. $p/n \rightarrow c \in [0,1)$. This makes some (although quite general) assumptions on the return distribution which we will ignore for now. If $c$ is close to one, then the sample GMV portfolios variance will explode. \textit{Estimation uncertainty dominates the diversification effect}. There are many solutions to the problem at hand (see e.g. \citet{lw17} or \citet{bodnar2021recent} and the references therein). Most of them have one common theme, random matrix theory and the use of some type of shrinkage estimator. We will give a quick summary of the former and a more verbose of the latter.

\section{Random Matrix Theory and Stieltjes transforms}
The subject of Random matrix theory (RMT) has many applications. It was originally developed in the context of quantum physics (see Ch. 1 of \citet{mehta2004random}). The theory and its applications has now developed. Many fields, such as combinatorics, computational biology, wireless communication and finance (see \citet{REF} for an overview) use these results.

The seminal work of \citet{wigner1993characteristic} originally went towards modeling the limiting spectral distribution of a $n \times n$ dimensional standard Gaussian random matrices $\bX$. The term "standard" might be a little misleading for statisticians as the matrix $\bX$ contains independent random variables although not identically distributed. The entries on the diagonal are $N(0,2)$ and the entries on the off-diagonal are $N(0,1)$. The empirical spectral distribution is equal to 
$$
F^{\bX/\sqrt{n}}(x)= \frac{1}{n} \sum_{i=1}^p \mathbbm{1}(\lambda_i \leq x)
$$ 
where $\lambda_i$ are the eigenvalues from the eigenvalue decomposition, see section \ref{subsec:cov_prec_matrix}.
The limiting spectral distribution can be shown to converge to
$$
F'(x) = \begin{cases}
\frac{1}{2\pi} \sqrt{4-x^2} & \text{ if } |x|\leq 2 \\
0 & \text{ otherwise.}
\end{cases}
$$
Although there are many interesting facts about the empirical spectral distribution and its limiting distribution one of the most surprising is the support of the limiting distribution. The normal distribution has unbounded support but the eigenvalues of $\bX$ converges to a distribution with bounded support. This is usually known as the Wigner's semi-circle law as the distribution is a semi-circle centered around zero. The convergence has been shown in probability  as well as almost surely, see Chapter 2 of \citet{bai2010spectral} for more information. The issue at hand is that this matrix is Hermitian and we are interested in real symmetric matrices. They have similar properties but are not equal. \citet{zbMATH03244317} extended this result to cover the important sample covariance matrix where we assume that $\bX$ contains .  

If 
<<MPlaw, eval=FALSE>>=
#X <- sqrt(3/5)* matrix(rt(600*300, df=10), ncol=600, nrow=300)
X <- matrix(rnorm(600*300), ncol=600, nrow=300)
df <- tibble(
  "values"= eigen(X%*%t(X), only.values = TRUE)$values,
  "type"="X t(X)"
) %>% bind_rows({
  tibble(
    "values"= eigen(t(X)%*% X, only.values = TRUE)$values,
    "type"="t(X) X"
  )
})
ggplot(df) +
  geom_histogram(aes(x=values, fill=type), bins = 100) +
  theme_minimal() +
  scale_fill_brewer("", type="qual", palette = 6)
@

The most fundemental 
\begin{itemize}
  \item Short on Stieltjes transforms.
	\item .
\end{itemize}

Although the two objects $\tr(\bS^{-1})$ and $\tr(\ones^\top \bS^{-1} \ones)$ may look similar their limiting objects will behave quite differently. This is due to the fact that the latter depend on the eigenvectors while the former does not. If we are working towards the . Theorem 1 of \citet{rubio2011spectral}
\section{Shrinkage estimators}
