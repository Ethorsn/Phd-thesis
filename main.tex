\documentclass[oneside]{book}\usepackage{knitr}

\input{header.tex}
\DeclareCiteCommand{\fullcite}
  {\usebibmacro{prenote}}
  {\clearfield{url}%
   \clearfield{pages}%
   \clearfield{pagetotal}%
   \clearfield{edition}%
   \clearfield{title}%
   \usedriver
     {\DeclareNameAlias{sortname}{default}}
     {\thefield{entrytype}}
     }
  {\multicitedelim}
  {\usebibmacro{postnote}}
%opening
\title{Optimal portfolios -- estimation and uncertainty assessment in the high-dimensional setting}
\author{Erik ThorsÃ©n}


\addbibresource{references.bib}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% \SweaveOpts{concordance=TRUE}


\maketitle



\section*{Acknowledgements}

First and foremost i would like to thank ...

\newpage
\section*{List of papers}
\nocite{bodnar2020sampling, javed2021tangency, bodnar2021dynamic, bodnar2021empirical}
This thesis is based on the following papers

\subsection*{Papers included in this thesis}
\begin{enumerate}[I]
  \item \textbf{Sampling Distribution of Optimal Portfolios in Small and Large Dimensions}
    \fullcite{bodnar2020sampling}
  \item \textbf{Tangency portfolio weights under a skew-normal model in small and large dimensions}
    \fullcite{javed2021tangency}
  \item \textbf{Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance Portfolio}
    \fullcite{bodnar2021dynamic}
  \item \textbf{Is the empirical out-of-sample variance an informative risk measure for the high-dimensional portfolios?}
    \fullcite{bodnar2021empirical}
  \item \textbf{Double shrinkage of the high-dimensional Global Minimum-Variance portfolio.}
    \fullcite{REF}
\end{enumerate}

\subsection*{Papers \& other research results which are not in included in this thesis}

\begin{itemize}
  \item The \textbf{DOSPortfolio} R-package which is published on CRAN. \href{https://CRAN.R-project.org/package=DOSPortfolio}{It is available here}.  
  \item \textbf{Bayesian Quantile-Based Portfolio Selection}
  \fullcite{bodnar2020quantile}
  \item \textbf{Volatility Sensitive Bayesian Estimation of Portfolio VaR and CVaR}
  \fullcite{bodnar2021bayesian}
  \item \textbf{Quantile-based optimal portfolio selection}
  \fullcite{bodnar2021quantile}
\end{itemize}
\textbf{Author contribution:} Thors{\'e}n E. contributed...

%%% test
%  
% \printbibliography[title={Subbibliography}, keyword=included_in_thesis, env=roman-numerals]


\tableofcontents
%%%%%% ------------------------------------------------------------------------
\chapter[Introduction]{Introduction -- Decisions, uncertainty and asset allocations}\label{ch:intro}
%%%%%% ------------------------------------------------------------------------
\input{chapters/01-Introduction}
%%%%%% ------------------------------------------------------------------------
\chapter[Modern portfolio theory]{Modern portfolio theory}\label{ch:MPT}
%%%%%% ------------------------------------------------------------------------



Modern Portfolio Theory (MPT) was introduced by \cite{markowitz1959portfolio}. 
In his seminal work he argued that any portfolio which simply maximizes its profit will result in a naive solution.
Very much like the example in Chapter \ref{ch:intro}. 
The future is unknown and (one of) the best model(s) we have for it is stochastic.
Investing all your capital in the asset with the highest return is not sensible if you do not know the future.
Such an investment will cause you to take an extreme amount of risk. 
Markowitz argued that any well diversified portfolio should be preferred to any non diversified portfolio. 
Such a portfolio can be obtained through many different procedures.
He proposed the use of the first two moments for the allocation problem.
If an asset has high return on average, it might make sense to invest a lot in it although not at the cost of large amounts of risk. 
If an asset is not risky, then it makes sense to invest in it.

We assume that the $p$-dimensional vector of asset returns $\bx$ is random with mean vector $\optn{E}(\bx)=\bmu$ and covariance matrix $\optn{Var}(\bx)=\bSigma$. 
The matrix $\bSigma$ is of dimensions $(p \times p)$. 
Although there are usually little restriction on $\bmu$ there are usually very specific restrictions on the covariance matrix $\bSigma$. 
Since the covariance matrix is a subject of its own we dedicate the next section to it and disregard these restrictions for now.
We will merely say that it is well behaved. 
The restrictions on the mean will be commented on below. 
Using the two moments for the asset returns the portfolio distribution $x = \bw^\top \bx$ has mean $\optn{E}(x)=\bw^\top \bmu$ and variance $\optn{Var}(x)=\bw^\top \bSigma \bw$. 
Let $\mu_0$ be the target return that the investor would like to achieve from his/hers portfolio and $\ones$ column vector of ones with appropriate dimensions. 
\textcite{markowitz1959portfolio} considered the following optimization problem
\begin{equation}\label{eqn:markowitz_optim}
\begin{aligned}
& \underset{\bw}{\text{minimize}} 
& & \bw^\top \bSigma \bw \\
& \text{subject to}
& & \bw^\top \ones = 1, \\
& && \bw^\top \bmu \geq \mu_0 \\
&&& w_i \geq 0, i=1,2,..,p
\end{aligned}
\end{equation}
This problem is a quadratic optimization problem with linear equality and inequality constraints. 
The objective is to minimize the variance of the portfolio. 
%A natural question is to ask whether or not that implies diversification? As it turns out, minimizing the portfolio variance will encourage diversification from the fact that $$
The constraint $\bw^\top \ones = 1$ essentially states that the investor must invest all available money. 
The weights are scaled according to the amount of cash invested.
The disposition is very different whenever an inequality is used rather than equality. 
As \textcite{hult2012risk} states, if $\bw^\top \ones \leq 1$, then the investor could be throwing money away since there is a lot of opportunity left in the market when investing.
The second constraint describes the investors expectations on the portfolio. 
As $\mu_0$ grows, the return of the portfolio will grow. 
However, that has implications for the objective. 
Increasing $\mu_0$ will change the amount of variance the portfolio can achieve. 
Depending on the value $\mu_0$ we would be accepting more risk, there is a risk-return trade off. 
The last constraint is rather simple though it can have quite large implications. 
It states that the weights can not be negative which means that we can only invest money we have. 
A negative value of $w_i$ in the $i$th asset is called a short position.
You borrow the asset from someone who owns it and then sell it, hoping that it will be cheaper in the future.
For certain types of investors this constraint can be limiting and for others its a must.
In this thesis, we exclude it altogether. That is, this thesis considers
\begin{equation}\label{eqn:mean_variance}
\begin{aligned}
& \underset{\bw}{\text{minimize}} 
& & \bw^\top \bSigma \bw \\
& \text{subject to}
& & \bw^\top \ones = 1 \\
& && \bw^\top \bmu \geq \mu_0 \\
\end{aligned}
\end{equation}
which is what we refer to the mean-variance optimization problem. The solution to this problem is very often stated in terms of another famous portfolio, namely the Global Minimum Variance (GMV) portfolio and its related quantities (see e.g. \textcite{Bodnar2009CaIotEFiEM, bodnar2013equivalence, bauder2018bayesian}). We will continue in the same manner. Let $\bSigma^{-1}$ denote the inverse matrix of $\bSigma$, e.g. $\bSigma^{-1}\bSigma = \bI$, and
\begin{equation}
	\bw_{GMV} := \frac{\bSigma^{-1}\ones}{\ones^\top \bSigma^{-1}\ones}, \; R_{GMV} :=\optn{E}(\bw_{GMV}^\top\bx) = \frac{\ones^\top\bSigma^{-1}\bmu}{\ones^\top \bSigma^{-1}\ones}, \;
	V_{GMV} := \optn{Var}(\bw_{GMV}^\top\bx) =\frac{1}{\ones^\top \bSigma^{-1}\ones}.
\end{equation}
The GMV portfolio can be obtained by letting $\mu_0=R_{GMV}$ or by removing the constraint $\bw^\top \bmu \geq \mu_0$. The solution to the mean-variance problem in \eqref{eqn:mean_variance} is equal to
\begin{equation}\label{eqn:mean_var_solution}
	\bw_{MV} = \frac{\bSigma^{-1}\ones}{\ones^\top \bSigma^{-1}\ones} + \frac{\mu_0 - R_{GMV}}{V_{GMV}} \bQ \bmu,\; \bQ = \bSigma^{-1} - \frac{\bSigma^{-1} \ones \ones^\top \bSigma^{-1}}{\ones^\top \bSigma^{-1} \ones}.
\end{equation}
The solution is a combination of two different portfolios, the GMV portfolio and the self-financing portfolio $\bQ \bmu$. The ratio $(\mu_0 - R_{GMV})/V_{GMV}$ acts as a weight to how much we should allocate in the self-financing portfolio. If we set $\mu_0$ equal to $\bmu^\top \bSigma^{-1} \ones / \ones^\top \bSigma^{-1} \ones$ then the portfolio is equal to the GMV portfolio. Excluding the constraint $\bw^\top \bmu \geq \mu_0$ all together results in the same solution. 

The moments of this portfolio is equal to
\begin{equation}\label{eqn:moments_mean_var_solution}
\optn{E}(\bw_{MV}^\top\bx) = R_{GMV} + \frac{\mu_0 - R_{GMV}}{V_{GMV}} \bmu^\top \bQ \bmu, \;
\optn{Var}(\bw_{MV}^\top\bx) =V_{GMV} + \left(\frac{\mu_0 - R_{GMV}}{V_{GMV}}\right)^2 \bmu^\top \bQ \bmu.
\end{equation}
From equation \eqref{eqn:moments_mean_var_solution} we can see that all values $\mu_0$ are rescaled according to the moments of the GMV portfolio. 
If the taget return $\mu_0$ is not $\mu_0>R_{GMV}$ then you are better off with the GMV portfolio in terms of return and risk.
However, if you choose a value $\mu_0>R_{GMV}$ then the portfolio return ''better'' than $R_{GMV}$. From the two expressions in \eqref{eqn:moments_mean_var_solution} we can see that as $\mu_0$ increases the risk grows quadratic in comparison to the mean which is linear. The portfolio adheres to diminishing marginal returns although in terms of risk and return. This relationship was discovered by \textcite{merton1972} which coined the expression ''the efficient frontier''. In Figure \ref{fig:mertons_efficient_frontier} on the left hand side, we show its behavior for two different portfolio sizes. Increasing the portfolio size shifts the location of the parabola, e.g. moves it to the left, which serves as an illustration of the diversification effect. There is no guarantee that an increase in in the portfolio dimension increases the return.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{figure/mertons_efficient_frontier-1} 

}

\caption[Two figures of efficient frontiers]{Two figures of efficient frontiers. The left Figure illustrates two different efficient frontiers for different portfolio sizes. The right figure we illustrate the efficient frontier and the capital market line which appears when a riskless asset is available. The stocks are randomly selected from the S\&P500. The individual means and standard deviations are displayed as points.}\label{fig:mertons_efficient_frontier}
\end{figure}

\end{knitrout}
Any point on any of the two lines in Figure \ref{fig:mertons_efficient_frontier} on the left hand side corresponds to a certain efficient and optimal portfolio with a specific value of $\mu_0$. The points are the individual stocks return and risk. You can obtain these by investing in everything you got in a specific stock. However, diversification is always better in terms of decreasing risk. No point will (theoretically) ever cross its efficient frontier. That can not happen. The efficient frontier is the best we can do with the stocks at hand, given the objective.

The right hand side figure of Figure \ref{fig:mertons_efficient_frontier}, displays an extension to the mean variance problem. It displays what happens when we include a riskless asset in the portfolio allocation problem. To introduce this option into our allocation problem we add the risk-free rate as part of the portfolio $w_0 r_f + \bw^\top \bx$ and optimize over $w_0$ as well. We assume that the risk-free rate is deterministic and therefore \eqref{eqn:mean_variance} is equal to 
\begin{equation}\label{eqn:mean_variance_riskfree}
\begin{aligned}
& \underset{\bw}{\text{minimize}} 
& & \bw^\top \bSigma \bw \\
& \text{subject to}
& & w_0 + \bw^\top \ones = 1 \\
& && w_0 r_f + \bw^\top \bmu = \tilde\mu_0 \\
\end{aligned}
\end{equation}
However, since $w_0 + \bw^\top \ones=1$ we substitute $w_0=1-\bw^\top \ones$ and solve the unconstrained optimization problem instead. Its solution is given by 
\begin{equation}\label{eqn:w_mean_variance_riskfree}
  \bw_{TP} = \frac{(\tilde\mu_0-r_f)}{(\bmu-r_f \ones)^\top \bSigma^{-1} (\bmu-r_f \ones)} \bSigma^{-1} (\bmu-r_f \ones).
\end{equation}
The portfolio defines the whole capital market line which was seen in Figure \ref{fig:mertons_efficient_frontier}. The portfolio has many interesting properties. If there is a risk-free asset we can increase the return and decrease the risk of our position in the market. This is most easily explained by the efficient frontier, displayed in Figure \ref{fig:mertons_efficient_frontier}. For a given level of return we can get at least the same amount of return or sometimes more. For a given level of return we can sometimes get the same or less risk! The same solution can be obtained from using what is known as the quadratic utility, defined as $\min_{\bw} \bw^\top \bmu - \gamma \bw^\top \bSigma \bw$ whose solution is given by $\bSigma^{-1} (\bmu-r_f \ones)/\gamma$. The difference is that $\frac{1}{\gamma} = \frac{(\tilde\mu_0-r_f)}{(\bmu-r_f \ones)^\top \bSigma^{-1} (\bmu-r_f \ones)}$. This is quite common in MPT, there are many portfolio allocation problems which result in the same solution, see .e.g. \citet{bodnar2013equivalence}.

Ever since the end of 2014, there has been a lack of a riskfree asset in Sweden.\footnote{See \href{https://www.riksbank.se/sv/statistik/sok-rantor--valutakurser/reporanta-in--och-utlaningsranta/}{Riksbanken}} The riskfree rate has been equal to or less than zero. Assuming that is true for our hypothetical investor, \eqref{eqn:w_mean_variance_riskfree} reduces to $\bw_{TP} = \tilde\bmu_0 \bSigma^{-1} \bmu / \bmu^\top \bSigma^{-1} \bmu$. The term $\bSigma^{-1} \bmu$ is also present in \eqref{eqn:mean_var_solution}, although hidden with the introduction of the matrix $\bQ$. With a little work, one can rewrite \eqref{eqn:mean_var_solution} as
$$
\left(1 - \frac{\bmu_0-\R}{\V} \frac{\R}{\V} \right) \bw_{GMV} + \frac{\bmu_0-\R}{\V} \bSigma^{-1} \bmu.
$$
There are two insights to be drawn from this equation. The first is that the weights on the efficient frontier is a combination of two portfolios, in this case the GMV and the tangency portfolio. This result is usually known as the Mutual fund theorem, see \textcite{tobin1958liquidity}. To study all the portfolios on the efficient frontier we only need to study these two portfolios. The second is that the true \textit{"tangent portfolio"} is given by the equation
$$
\tilde\mu_0 = \R + \frac{\mu_0-\R}{\V} s  
$$
which is where the efficient frontier and the capital market line meet. Any tangency portfolio with $\bmu_0\leq \R + \frac{\mu_0-\R}{\V} s$ will be "more efficient" than the efficient frontier if there is a risk free rate. However, its not always the case that we want to optimize the amount of cash held in the riskfree asset. Given that cash is free, we should most likely borrow as much as possible to invest in the market. 

All of MPT use the inverse covariance matrix. In the next section we devote some attention to the assumption we make on the covariance matrix.  

\section{Relationship between assets and the (inverse) Covariance matrix}\label{subsec:cov_prec_matrix}
%%% ----------------------
The covariance matrix $\bSigma$ and the precision matrix $\bSigma^{-1}$ are fundemental to mean-variance portfolios. In this section we discuss the restrictions we place on the covariance matrix and what the precision matrix actually represent. 

For a vector $\bx$ with finite second moment, the covariance matrix is defined as $\bSigma=\optn{E}((\bx - \bmu)(\bx - \bmu)^\top)$. 
It contains the variances of each individual element of $\bx$ on the diagonal as well as the covariance between every pair of elements on the off-diagonal. 
That is, each diagonal element corresponds to the univariate case with variance equal to $\optn{E}((x_i - \mu_i)^2)$. 
In the univariate case, a distribution is usually called degenerate or singular if the variance is equal to zero. 
In the multivariate case. the covariance matrix can be singular on a number of occasions. 
It is not limited to the diagonal elements.
This is due to the fact that we involve covariances on the off-diagonal and we are therefore forced to work with a broader definition.  
Since we work with real matrices in this thesis, we limit the definition accordingly. 
From \textcite[ch 14.2]{harville1997matrix} we say that a real symmetric $p\times p$ matrix $\bA$ is called 
\begin{itemize}
	\item positive definite if $\bz^\top \bA \bz > 0$
	\item positive semi-definite if $\bz^\top \bA \bz \geq 0$
\end{itemize}
for all nonzero vectors $\bz \in \mathbbm{R}^p$.
We will use $\bA > 0$ or $\bA \geq 0$ to indicate positive or semi-positive definiteness of the matrix $\bA$. 
In the multivariate case we need to assert that a quadratic form is (strictly) positive in comparison to the univariate setting where we can observe it through the variance. 
Positive- or semi-positive definite can be quite cumbersome to work with. 
We need to assert that the conditions holds for all vectors $\bz$. 
One necessary condition for a matrix to be positive definite can be derived using the eigenvalues of a matrix and its eigenvalue decomposition. 
As described in \textcite[ch. 21]{harville1997matrix}, an eigenvalue (or characteristic root) $\lambda$ is the solution to 
\begin{definition}\label{def:eigenvalue} 
	Let $\bA$ be a $p\times p$ matrix. The characteristic roots (with multiplicity) are given by the solutions to
	\begin{equation*}
		\left|\bA - \lambda \bI\right| = 0
	\end{equation*}
	where $|\cdot|$ is the determinant of a matrix.
\end{definition} 
Let $\lambda_i$, $i=1,2,...,p$, denote the \textit{ordered} eigenvalues of the matrix $\bA$ such that $\lambda_1\geq \lambda_2 \geq ... \geq \lambda_p$.
Given an eigenvalue, the eigenvectors $\bu_i$ are defined by $\bA \bu_i = \lambda_i \bu_i$, $i=1,2,...,p$. 
Let $\boldsymbol{\Lambda} = \operatorname{diag}(\lambda_1, \lambda_2,...,\lambda_p)$ and $\bU= (\bu_1^\top, \bu_2^\top, ..., \bu_p^\top)^\top$. It might happen that some eigenvalues are equal, which implies that some eigenvectors have the same multiplicity.
Using the relation between eigenvalues and their eigenvectors we can derive the eigenvalue (or spectral) decomposition of a symmetric matrix 
\begin{equation}\label{eqn:eigenvalue_decomp}
	\bA = \bU \boldsymbol{\Lambda} \bU^{-1}.
\end{equation}
Since $\bA$ is symmetric it also holds that $\bU^{-1} = \bU^\top$.
A necessary condition for a matrix to be positive definite can be directly obtained from the eigenvalue decomposition. 
Let $\bz\in \mathbbm{R}^p$ and $\by := \bU^{\top} \bz \in \mathbbm{R}^p$, then $\bz^\top \bA \bz = \bz^\top \bU \boldsymbol{\Lambda} \bU ^{\top} \bz = \by^\top \boldsymbol{\Lambda} \by = \sum_i^p \lambda_i y_i^2$ which is a second degree polynomial. 
If the eigenvalues are all positive then necessarily the matrix is positive definite. 
If there are some eigenvalues which are zero then the matrix is semi-positive definite. 

In all papers of this thesis we assume that the true covariance matrix is positive definite. 
The assumption has quite a deep economical interpretation.
If one (or more) eigenvalue(s) are zero then there is a possibility to construct a portfolio which does not contain any risk with a potentially positive return. 
An opportunity which should not exist unless the elements of $\bmu$ are all zero.
Assume $\lambda_p=0$, let $\bu_p$ be its eigenvector and set $\bw = \bu_p / \sum_i^p u_{ip}$. 
The variance of that portfolio is zero since all eigenvectors are orthonormal and its mean is $\bw^\top \bmu$ which can be non-zero unless the elements of $\bmu$ are all zero.
If the true covariance matrix is not positive definite there might exist arbitrage opportunities, e.g. the possibility of making profit without taking any risk.

The eigenvalue decomposition is very useful.
It provides a simple way to construct inverses, which is very important for MPT as seen in \eqref{eqn:mean_var_solution}.
We claim that $\bB = \bU \boldsymbol{\Lambda}^{-1} \bU^{-1}$ is a valid inverse which is easy to verify since $\bB \bA = \bU \boldsymbol{\Lambda}^{-1} \bU^{-1} \bU \boldsymbol{\Lambda} \bU^{-1} = \bI$. 
To study the inverse we can study the inverse of the eigenvalues.
%Secondly, it contains a lot of information that might not be available at first glance. 
%If $\bA$ is a covariance matrix then it contains variances and covariances, describing relations between random variables. 
%The eigenvectors are rotations that try to capture as much variation as possible along its axis.
%The eigenvalues is the variation along the eigenvectors axis. 
%They describes how the system behaves and not the individual elements and their inverse values describe how the precision matrix behaves.

%%%%%% ------------------------------------------------------------------------
\chapter[Estimation \& models]{Estimation and statistical models}\label{ch:estim}
%%%%%% ------------------------------------------------------------------------


To \textit{pratically} use the portfolios described by \eqref{eqn:mean_var_solution} we have to specify $\bmu$ and $\bSigma$. 
This is not really feasible as we might have many assets. 
We might have a opinion of what they should be but we dont know. 
Furthermore, even if you have an informed opinion of the parameters the potential loss of using those exact parameters might be paramount. 
We usually want to rely on data to estimate the parameters of interest. 
In this thesis we never use the asset prices themselves but a transformation of the relative differences, that is, their simple- and log returns. 
Let $p_{i,t}$ be the asset price of the $i$th asset at time $t$. 
The simple return is defined as $r_{i,t} := (p_{i,t}-p_{i,t-1})/p_{i,t-1}$ and the log return is then defined as $y_{i,t} := \log(r_{i,t} + 1)$ and $\by_t=(y_{1,t},y_{2,t},..., y_{p,t})$.
A portfolio with $p$ assets is then modeled as $\sum_{i=1}^p w_i y_{i,t} = \bw^\top \by_t$ where $\bw=(w_1, ..., w_p)$ are the portfolio weights.
Notice that this is an approximation. 
In reality we would want to work with $\sum_{i=1}^p w_i r_{i,t}$ (or even $\sum_{i=1}^p w_i z_{i,t}$) since it is additive in the number of assets. 
However, logarithmic returns are additive in time which can be desirable. 
Compounding returns is simple addition and the approximation can make the statistical analysis more tractable. 
The difference between the two approaches is very small if the (log) returns are small, which is often true for financial assets, see \citet[p. 5]{tsay2005analysis}. 

Assuming that we have a model for the log returns there are many ways of estimating $\bmu$ and $\bSigma$.
The most simple and perhaps the most robust method is using method of moments (MM) \cite{REF}. 
Let $\bY = (\by_1, \by_2, ..., \by_n)$ be a sample of log returns.
Using these, we replace $\bmu$ with the sample mean and $\bSigma$ with the sample covariance matrix, e.g.
$$
\byb = \frac{1}{n} \sum_i^n \byb_i, \; \bS = \frac{1}{n}\bY \left(\bI_n - \frac{1}{n} \ones_n \ones_n^\top \right) \bY^\top.
$$
This is always a feasible approach assuming that the first two moments actually exist. 
However, it introduces some issues.
If our sample size $n$ is small, then our estimates are naturally imprecise. 
Furthermore, MPT relies on $\bS^{-1}$ and not $\bS$ which demands that $n>p$. 
Its natural to ask; does an imprecise estimate of the covariance matrix provide an equally imprecise estimate of the inverse?
It turns out to be a very hard question to answer. In some simple cases the answer is no, sometimes its worse. 
It is therefore very important to understand the implications of not using the true parameters but their sample counterparts.
There are many approaches to this but we take the bottom-up approach. 
If we assume that the asset returns $\bx_t$ follow some distribution then we can perhaps derive statistical properties for $\byb$ and $\bS$.
In turn, we need to derive the properties of $\bS^{-1}$ and in the end all the transforms given by \eqref{eqn:mean_var_solution}.

One of the most fundamental models for asset returns is the multivariate normal distribution. 
Since most of the distributions we work with are matrix variate, we will state the matrix variate normal distribution. 
It is slightly more general but capture much more dynamics.
% Multivariate normal distribution
\begin{definition}[Definition 2.2.1 \citet{GuptaNagar2000}]
	The random matrix $\bY$ $(p \times n)$ is said to have a matrix variate normal distribution with mean matrix $\bM$ and covariance matrix $\bSigma \otimes \bGamma$ where $\bSigma > 0$ is of dimension $(p \times p)$ and $\bGamma >0$ is of dimension $(n \times n)$, if $\optn{vec}(\bY^\top) \sim N_{np}(\optn{vec}(\bM^\top), \bSigma \otimes \bGamma)$.
\end{definition}
The multivariate normal distribution is a simple special case of it with $\bGamma = \bI$. 
%It results it the following density
%\begin{equation}\label{eqn:multi_density}
%	\frac{|\bSigma|^{-n/2}}{2\pi} \exp \left\{-\frac{1}{2} \left(\by - \bmu \right)^\top\bSigma^{-1}\left(\by - \bmu \right) \right\}
%\end{equation}
%where $|\bB|$ is the determinant of the matrix $\bB$.

The multivariate normal distribution has often been criticized as a model for the asset return. 
Daily returns are not symmetric, exhibit fatter tails than the normal distribution and often show volatility clustering \cite{cont2001empirical}.  
However, it is often argued that that returns on a lower frequency such as weekly, monthly or quarterly should be close to normal \cite{REF}.
The model can therefore be thought of an investor which invests more seldom than daily.
That does not mean that they cant observe the results of the market on a higher frequency than they invest!
%by the Central Limit Theorem we also know that for large enough $k$, $\tilde\by_t=\sum_{i=t-k}^t \by_i$ should be close to normal. 
%Due to the fact that we work with log returns one can see that $\sum_{i=t-k}^t \by_i = \sum_{i=t-k}^t \log \bz_i - \log \bz_{i-1}= \log \bz_{t} - \log \bz_{t-k-1}$. Returns on a lower frequency are merely differences in prices
%these we can simplify the expression and  that returns on a lower frequency such as weekly, monthly or quarterly should be close to normal. 

in the univariate case we have that the sample variance follows a chi-square distribution. If the returns follow a multivariate normal distribution then $\bS$ follows what is known as a Wishart distribution. It is essentially a generalization of the chi-square distribution. We state its density below.
% Wishart
\begin{definition}[Definition 3.2.1 \citet{GuptaNagar2000}]
	A $p\times p$ random symmetric positive definite matrix $\bS$ is said to have a Wishart distribution with parameters $p, n$ ($n\geq p$) and $\bSigma > 0$, $(p \times p)$ written as $\bS \sim W_p(n, \bSigma)$ if its p.d.f. is given by
	\begin{equation}\label{eqn:wishart_density}
  	\frac{|\bS|^{(n-p-1)/2} |\bSigma|^{- n/2} }{2^{pn/2} \Gamma_p (n/2) } \exp\left\{-\frac{1}{2} \operatorname{tr}(\bSigma^{-1}\bS)  \right\}
	\end{equation}
	where $ \Gamma_p (\cdot) $ is the multivariate gamma function.
\end{definition}
In comparison to the normal distribution the Wishart distribution is used very frequently as a model for covariance matrices although in a slightly different context.
The model is very often used for realized covariance matrices, see \cite{barndorff2004econometric} or \citet{alfelt2021modeling}.
A realized covariance matrix is an estimates of the volatility process from returns on a much higher frequency than we work with in this thesis.
From Theorem 3.3.6 we know that if $\bY \sim N_{p,n}(\bmu \ones_n^\top, \bSigma \otimes \bI_n)$ then $n\bS \sim W(n-1, \bSigma)$, so working bottom up we can get a model for the parameters of the model.
As previously stated, MPT works with inverse covariance matrices and not the covariance matrix itself. Thankfully, the Wishart distribution has an inverse counterpart.
% Inverse Wishart
\begin{definition}[Definition 3.4.1  \citet{GuptaNagar2000}]
	A random matrix $\bV$ is said to be distributed as an inverted Wishart distribution with $m$ degrees of freedom and parameter matrix $\bGamma$ $(p \times p)$, denoted by $\bV \sim IW_p(m, \bGamma)$, if its density is given by
	\begin{equation}\label{eqn:inverse_wishart}
	\frac{2^{-(m-p-1)p/2} |\bGamma|^{(m-p-1)/2} }{\Gamma_p ((m-p-1)/2) |\bV|^{m/2}} \exp\left\{ -\frac{1}{2} \bV^{-1} \bGamma \right\}, \; m > 2p, \bV, \bGamma > 0.
	\end{equation}
\end{definition}
To once more connect to the univariate setting, the inverted sample variance follows an inverted chi-square distribution which is a special case of the inverted gamma distribution.
It is only natural that the inverted Wishart matrix is a matrix variate generalization of the inverted gamma distribution (p. 111 \citet{GuptaNagar2000}). 
It demands quite specific constraints on the parameters of the model, especially $m > 2p$.
To come back to out question we posed in the beginning of this section, does inverses change uncertainty? 
We can at least get a hint that \textit{something} changes with the properties of $\bS$ when taking inverses.
From Theorem 3.3.7, 3.4.1 and Theorem 3.4.3 of \citet{GuptaNagar2000} we have that
$$
\optn{E}\left[\bS\right] = \frac{n-1}{n} \bSigma, \; 
\optn{E}\left[\bS^{-1}\right] = \frac{n}{n-p-2}\bSigma^{-1}.
$$
If $n$ is sufficiently large, than the sample covariance matrix is (close to) unbiased.
That is not necessarily the case for its inverse.
If we believe in diversification then we should own many assets, e.g. $p$ should be large. 
That in turn could make the estimator very biased!
The answer is yes, inverses can potentially make matters worse.
To make things worse, the noise in the sample mean can be extremely large in comparison to the noise in the sample covariance matrix \cite{REF}.
The weights from \eqref{eqn:mean_var_solution} will be much more noisy whenever $\mu_0 \neq \R$. 
Its one of the most common motivations for considering the GMV portfolio, its very hard to estimate the mean \citep{golosnoy2019exponential}.  
If we want to include a target return then we should aim to incorporate this uncertainty it in our analysis.

These three distributions have many interesting and convenient mathematical properties which are repeatedly used in Paper 1 and 2 of this thesis.
Most important are how partitions of the inverse sample covariance matrix and their conditional distributions behave.
These papers heavily rely on the properties of the normal and Wishart distribution, essentially repeated use of chapter 2 and 3 of \citet{GuptaNagar2000}.

It has been well established that for higher frequency returns the normal assumption is limiting. 
The next, and perhaps most common feature, to include is skewness of the asset returns and its effect on portfolios. 
A $p$ dimensional Closed Skew Normal (CSN) random vector $\bz$ has density
\begin{equation}
  f_\bz(\ba; \bmu, \bSigma, \bD, \bv, \Delta) = C \phi_p(\ba; \bmu, \bSigma) \Phi_q(\bD(\ba-\bmu); \bv, \Delta)
\end{equation}
where $C$ is a normalization constant and $\bmu, \bSigma, \bD, \bv$ and $\Delta$ are parameters of appropriate dimensions. Its matrix variate counterpart is simply defined through the vec operator. We have that
% Closed skew normal distribution
\begin{definition}[Definition 3.1 \citet{dominguez2007matrix}]
  A random matrix $\bY$ $(p \times n)$ is said to have a matrix variate closed skew-normal distribution with parameters $\bM$ $(p \times n)$, $\bA$ $(np \times np)$, $\bB$ $(nq \times mp)$, $\bL$ $(q \times m)$ and $\bQ$ $(mq \times mq)$, with $\bS > 0$ and $\bQ>0$ if
  \begin{equation}
    \optn{vec}(\bY^\top) \sim CSN_{pm, qn}\left(\optn{vec}(\bM^\top), \bA, \bB, \optn{vec}(\bL^\top), \bQ\right)
  \end{equation}
\end{definition} 
The closed skew-normal distribution is heavily parametrized. 
It has the potential to fix a several parameters for each point in the sample.
However, it also puts a heavy restriction on some of them, as they need to be positive definite.
The parameter $\bA$ might be interpreted as a covariance matrix although that is a simplification.
Its something more, capturing variance along \textit{both} axis of the matrix $\bY$.
There is also some type of dependence between $\bA$ and how skewness will be observed, by the fact that moments include \textit{almost all of the parameters} (see e.g. Proposition 3.2 \citet{dominguez2007matrix}).
It is variance in time and among assets, for our application.
From the stochastic representation in Proposition 2.1 \citet{dominguez2007matrix} one can, after some thought and derivations
\footnote{\colr{The proposition is a little bit misleading as there seem to be a absolute value missing and the parameter $\bv$ appears as random but is also part of the parametrization. If the reader is interested in it go through the steps that begins at the end of page 7 in the same reference (page 1606).}}
realize that the skewness is introduced as shocks to the mean.
The mean is stochastic. 
The overzealous parametrisation and difficulty in estimating the parameters made us choose a special case of it for paper 2 of this thesis.
We work with a special case of the distribution where $q=m=1$.

% 
The last model we consider in this thesis is the most general.
Its also the model that has the least amount of interesting properties in itself.
It is the following location and scale model
\begin{equation}\label{eqn:location_scale_model}
\bY \eqdist \bmu \ones^\top_n + \bSigma^{1/2} \bZ.
\end{equation}
where $\eqdist$ stands for equality in distribution and $\bZ = \{z_{ij}\}$, $i=1,2,...,p$, $j=1,2,...,n$.
Although the model can capture many types of return distributions, such as skew heavy tailed sometimes even heteroscedasticity, there is very little to say about it.
The first and foremost assumption is that it is a location and scale model.
In this thesis we very often assume moment conditions on the "residuals" $z_{ij}$ such as finite fourth moment or potentially $4+\epsilon$ finite moment, for some $\epsilon>0$.
The difference between finite fourth moment and $4+\epsilon$ is most often the claims of convergence we can show.
With the slightly more stringent assumption we can make claims about almost sure convergence and with finite fourth we can usually make claims about convergence in probability \citet{REF}.

Common to all of finance and econometrics is usually simulation. 
To understand a system we usually simulate from it and compute the quantity of interest, rinse and then repeat.
For some methods simulations are the only way we can compute the quantities of interest.
It can therefore be very important that simulations are fast.

\section{Simulations, inverses and why stochastic representations are valuable}
Assume that the investor cares about simulations, is interested in the GMV portfolio and for the train of thought that $\bY \sim N_{p,n}(\bmu \ones_n^\top, \bSigma \otimes \bI_n)$. To simulate from the sampling distribution of the variance of the GMV portfolio we need to 
\begin{enumerate}
  \item Simulate $\bY$ and construct $\bS$
  \item Invert $\bS$
  \item Compute $\hV$
\end{enumerate}
The second step is notoriously demanding.
The default method to use in R is `solve` which is a wrapper for certain LAPACK\footnote{For the interested reader \url{https://www.netlib.org/lapack/}} functions.
The inverse itself takes $2p^3$ flops (cpu cycles), which is not cheap \citet[ch 14]{higham2002accuracy}.
If $p$ is large then simulation of the quantity $\hV$ will be extremely cumbersome.
Another method is R's `chol2inv` which relies on the cholesky decomposition. 
In theory it should be faster but demands that we compute the cholesky decomposition.
The last two options that are available is to simulate $\bS$ directly or to derive the stochastic representation of $\hV$ directly.
Paper 1 and 2 goes into great detail to derive the stochastic representation of different quantities of optimal portfolios. 
One of them is the sample variance of the GMV portfolio.
By Theorem 1 \citet{bodnar2020sampling} we know that if $\bY \sim N_{p,n}(\bmu \ones_n^\top, \bSigma \otimes \bI_n)$ then $\hV \sim \V\xi /(n-1)$ where $\xi \sim \chi^2_{n-p}$.
We can omit inversions all together.
In \ref{benchmark} we present R-code which implements a small benchmark to highlight why these types of representations can be really valuable.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\captionof{chunk}{R-code for benchmarking different simulation approaches of the variance of the GMV portfolio.}\label{benchmark}\begin{alltt}
\hlcom{# setup}
\hlstd{p} \hlkwb{<-} \hlnum{150}
\hlstd{n} \hlkwb{<-} \hlnum{250}
\hlstd{Sigma} \hlkwb{<-} \hlstd{HDShOP}\hlopt{::}\hlkwd{RandCovMtrx}\hlstd{(p)}
\hlstd{Sigma_chol} \hlkwb{<-} \hlkwd{chol}\hlstd{(Sigma)}
\hlstd{mu} \hlkwb{<-} \hlkwd{runif}\hlstd{(p,} \hlopt{-}\hlnum{0.1}\hlstd{,} \hlnum{0.1}\hlstd{)}
\hlstd{Sigma_inv} \hlkwb{<-} \hlkwd{solve}\hlstd{(Sigma)}
\hlstd{V_GMV} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlkwd{sum}\hlstd{(Sigma_inv)}
\hlcom{# microbechmark}
\hlstd{result} \hlkwb{<-} \hlkwd{microbenchmark}\hlstd{(}
  \hlcom{# Simulate Y directly, construct S, invert and compute GMV variance}
  \hlkwc{`Scenario 1`} \hlstd{= \{}
    \hlstd{Y} \hlkwb{<-} \hlstd{mu} \hlopt{%*%} \hlkwd{t}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,n))} \hlopt{+} \hlkwd{t}\hlstd{(Sigma_chol)}\hlopt{%*%}\hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{p),} \hlkwc{ncol} \hlstd{= n)}
    \hlstd{S} \hlkwb{<-} \hlkwd{var}\hlstd{(}\hlkwd{t}\hlstd{(Y))}
    \hlnum{1}\hlopt{/}\hlkwd{sum}\hlstd{(}\hlkwd{solve}\hlstd{(S))}
  \hlstd{\},}
  \hlcom{# Simulate Y directly, construct S and its chol. decomp., use chol2inv and}
  \hlcom{# compute GMV variance}
  \hlkwc{`Scenario 2`} \hlstd{= \{}
    \hlstd{Y} \hlkwb{<-} \hlstd{mu} \hlopt{%*%} \hlkwd{t}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,n))} \hlopt{+} \hlkwd{t}\hlstd{(Sigma_chol)}\hlopt{%*%}\hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(n}\hlopt{*}\hlstd{p),} \hlkwc{ncol} \hlstd{= n)}
    \hlstd{S} \hlkwb{<-} \hlkwd{var}\hlstd{(}\hlkwd{t}\hlstd{(Y))}
    \hlstd{S_chol} \hlkwb{<-} \hlkwd{chol}\hlstd{(S)}
    \hlnum{1}\hlopt{/}\hlkwd{sum}\hlstd{(}\hlkwd{chol2inv}\hlstd{(S_chol))}
  \hlstd{\},}
  \hlcom{# Simulate S directly, invert and compute GMV variance}
  \hlkwc{`Scenario 3`} \hlstd{= \{}
    \hlstd{S} \hlkwb{<-} \hlkwd{rWishart}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{df}\hlstd{=n}\hlopt{-}\hlnum{1}\hlstd{,} \hlkwc{Sigma}\hlstd{=Sigma)[,,}\hlnum{1}\hlstd{]}
    \hlnum{1}\hlopt{/}\hlkwd{sum}\hlstd{(}\hlkwd{solve}\hlstd{(S))}
  \hlstd{\},}
  \hlcom{# Simulate directly from the GMV sample variance distribution.}
  \hlkwc{`Scenario 4`} \hlstd{= V_GMV}\hlopt{/}\hlstd{(n}\hlopt{-}\hlnum{1}\hlstd{)} \hlopt{*} \hlkwd{rchisq}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{df}\hlstd{=n}\hlopt{-}\hlstd{p),}
  \hlkwc{times}\hlstd{=}\hlnum{1000}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{figure/microbenchmark_output-1} 

}

\caption[Small benchmark of 1000 iterations to showcase the difference in performance between different simulation methods of the variance of the GMV portfolio]{Small benchmark of 1000 iterations to showcase the difference in performance between different simulation methods of the variance of the GMV portfolio.}\label{fig:microbenchmark_output}
\end{figure}

\end{knitrout}

Scenario 4 uses the stochastic representation. 
Its execution time varies a lot more than the three former which is why the distribution is so flat.
However, the execution time of scenario 4 is much smaller than the former strategies.
It is quite clear that it is the fastest. 
The conclusion is that inversions are very cumbersome to deal with and take a lot of time regardless if we use the cholesky decomposition or not.
Its can also be a very unstable operation, especially if the matrix you are trying to invert is close to singular.

So far we have assumed that all we wanted to use is $\bS$, $\byb$ or simply $\hbw_{GMV}$.
That is of course a simplification and not always the case.
As we previously mentioned both $\bS$ and $\byb$ can be noisy estimators with the former being less noisy than the latter. 
Furthermore, we saw that if $p$ is comparable to $n$ but $n>p$ then the expectation of the inverse Wishart distribution is very biased.
We can cope with that through two strategies. 
The first is to derive the actual uncertainty and sample distributions of the quantities of interest, which we do in Paper 1 and 2.
The second is to use other estimators which introduce some bias of our own.
By introducing some bias in the estimator we can reduce the variance.
It is something of utmost importance if we believe in diversification which we will go into detail in the next section.

%%%%%% ------------------------------------------------------------------------
\chapter[Diversification \& shrinkage]{Diversification, infinitely many assets and shrinkage estimators}\label{ch:highdim}
%%%%%% ------------------------------------------------------------------------


In the previous chapter we presented one specific way of estimating the sample covariance matrix.
We also presented certain properties under certain conditions and/or statistical models.
%If we hold $p$ constant and let $n$ grow the sample covariance matrix is consistent. 
If we have a lot of data on the assets that we are trying to invest in then we can most often be certain that we will hold the correct portfolio.
Our estimated portfolio will be consistent, e.g. it estimates the correct object of interest. 
Furthermore, since diversification is one, if not the best, risk management tool there is, we want our asset universe to be big.
If we believe in diversification then $p$ should be large. 
It should, in theory, decrease the risk (variance) of the portfolio. 
That is not always the case.
By introducing one new asset to our portfolio of size $p$ we need to estimate all covariances for that asset in the sample covariance matrix. 
They will constitute an additional $p+1$ quantities. 
The sample covariance matrix suffers from the curse of dimensionality. 
In terms of estimation uncertainty, this does not scale well.
From \citet{bodnar2016optimal} Proposition 2.2 we know that $\hV \rightarrow \V/(1-c)$ whenever $p,n \rightarrow \infty$ s.t. $p/n \rightarrow c \in [0,1)$. If $c$ is close to one, then the sample GMV portfolios variance will explode. \textit{Estimation uncertainty dominates the diversification effect}. There are many solutions to the problem at hand (see e.g. \citet{lw17} or \citet{bodnar2021recent} and the references therein). We will focus on some topics in Random Matrix Theory (RMT) and the use of some type of shrinkage estimator. Both subjects are grand. Our aim is to provide a small introduction to them in the following sections.

\section{A short introduction to random matrix theory and the Stieltjes transform}
The subject of Random matrix theory (RMT) has many applications. It was originally developed in the context of quantum physics (see Ch. 1 of \citet{mehta2004random}). The theory and its applications has since then developed quite a lot. Many fields, such as combinatorics, computational biology, wireless communication and finance (see \citet{REF} for an overview) use these results. One of the seminal work in RMT was made by \citet{wigner1993characteristic}. He originally modeled the limiting spectral distribution of an $p \times p$ dimensional standard Gaussian random matrices $\bX$. The term "standard" might be a little misleading for statisticians as the matrix $\bX$ contains independent random variables although not identically distributed. The entries on the diagonal are $N(0,2)$ and the entries on the off-diagonal are $N(0,1)$. However, the more generalized definition only demands that the matrix $\bX$ is Hermitian and its entries on the diagonal or above the diagonal are independent. We define the empirical spectral distribution (ESD) of a matrix $\bA$ as
$$
F^{\bA}(x)= \frac{1}{p} \sum_{i=1}^p \mathbbm{1}(\lambda_i \leq x)
$$ 
where $\lambda_i$ are the eigenvalues from the eigenvalue decomposition, see section \ref{subsec:cov_prec_matrix}. The limit, in this case, is taken as $p \rightarrow \infty$ which implies that $\bA$ will have infinitely many columns as well as rows!
The limiting spectral distribution of $\bX$ can be shown to converge to (see Chapter 2 of \citet{bai2010spectral})
$$
F'(x) = \begin{cases}
\frac{1}{2\pi} \sqrt{4-x^2} & \text{ if } |x|\leq 2 \\
0 & \text{ otherwise.}
\end{cases}
$$
There are many interesting facts about the empirical spectral distribution and its limiting distribution. One of the most interesting is the support of the limiting distribution. The normal distribution has unbounded support but the eigenvalues of $\bX$ converges to a distribution with bounded support (see \citet{livan2018introduction} for a good introduction on why this is). \citet{zbMATH03244317} extended the result of \citet{wigner1993characteristic} to the sample covariance matrix. Assume that $\bX$ is a $p \times n$ matrix that contains i.i.d random variables with zero mean and variance equal to $1$. The limit is now taken over the two quantities $p$ and $n$ at the same time, such that $p/n$ stays constant. We usually call this ratio the concentration ratio $c$. In this introduction we assume that $c<1$. The limiting spectral distribution of $\bS=\frac{1}{n} \bX \bX^\top$ was then shown to be
$$
F'(x) = \begin{cases}
\frac{1}{2\pi x c} \sqrt{(b-x)(x-a)} & \text{ if } a \leq x \leq b\\
0 & \text{ otherwise.}
\end{cases}
$$
where $a=(1-\sqrt{c})^2$ and $b=(1+\sqrt{c})^2$. The distribution has, once again, bounded support! The eigenvalues seem to attract each other. Although the sample covariance matrix appears very often in the context of MPT, its not usually the object of interest. We are interested in its inverse, as we discussed in chapter \ref{ch:MPT}. However, the Stieltjes transform can help us with that. The Stieltjes transform of a function $F: \mathbbm{R} \rightarrow \mathbbm{R}$ is defined as 
\begin{equation}\label{eqn:stieltjes}
m^F(z) = \int \frac{1}{x-z}dF(x)
\end{equation}
where $z \in \{z \in \mathbbm{C}: \mathbbm{Im}(z)>0 \}$. The Stieltjes has many useful properties. If we know the Stieltjes transform, then we can also derive the spectral distribution $F$ by its inversion formula. We also have pointwise convergence (see appendix B.2 of \citet{bai2010spectral}). Using the results from RMT in MPT we take a sample covariance matrix $\bS$ with ESD $F_n(x)$ and note that
\begin{equation}
\frac{1}{p}\tr \left( \bS^{-1} \right) = \lim_{z\rightarrow 0^+} \frac{1}{p} \tr \left( (\Lambda -z\bI)^{-1} \right) = \lim_{z\rightarrow 0^+} \int_0^\infty \frac{1}{x - z} dF_n(x) = \lim_{z\rightarrow 0^+} m^{F_n}(z).
\end{equation}
If we are interested in the limiting properties of traces of inverse sample covariance matrices, we can investigate the properties of the Stieltjes transform. However, to make matters slightly worse, we are most often (at least in this thesis) interested in quadratic or bilinear forms where the inverse sample covariance matrix is present. Examples are $\ones^\top \bS^{-1} \ones$ or $\ones^\top \bS^{-1} \bb$ for some vector $\bb$. Altough $\tr(\bS^{-1})$ and $\ones^\top \bS^{-1} \ones$ may look similar, their limiting objects can behave quite differently. This is due to the fact that the former does not depend on the eigenvectors while latter does. \citet{rubio2011spectral} showed the following theorem which allows us to handle limiting objects on this specific form
\begin{theorem}[Theorem 1 of \citet{rubio2011spectral}]
\begin{enumerate}[(a)]
  \item $\bX$ is an $p \times n$ random matrix such that the entier of $\sqrt{n}\bX$ are i.i.d complex random variables with mean 0, variance 1 and finite $8+\epsilon$ moment, for some $\epsilon > 0$.
  \item $\bA$ and $\mathbf{R}$ are $p \times n$ hermitian nonnegative definite matrices, with the spectral norm (denoted by $||\cdot||$) of $\mathbf{R}$ being bounded uniformly in $p$, and $\mathbf{T}$ is an $n \times n$ diagonal matrix with real nonegative entries  uniformly bounded in $n$.
  \item $\bB=\bA + \bR^{1/2} \bX \bT \bX^H \bR^{1/2}$, where $\bR^{1/2}$ is the nonnegative definite square root of $\bR$.
  \item $\bTheta$ is an arbitrary nonrandom $p \times p$ matrix, whose trace norm (i.e., $\tr((\bTheta^H \bTheta)^{1/2}):=||\bTheta||_{tr}$) is bounded uniformly in $p$.
\end{enumerate}
Then, with probability 1, for each $z\in \mathbbm{C}-\mathbbm{R}^+$, as $n=n(p) \rightarrow \infty$ such that $0<\lim\inf c_p<\lim \sup c_p < \infty$, with $c_p = p/n$
\begin{equation}
  \tr\left(\bTheta\left( \left(\bB - z\bI\right)^{-1} - \left( \bA + x_p(e_p)\bR - z\bI \right)^{-1} \right) \right) \rightarrow 0
\end{equation}
where $x_p(e_p)$ is defined as
\begin{equation}
  x_p(e_p) = \frac{1}{n}\tr \left( \bT \left(\bI_n + c_p e_p \bT \right)^{-1} \right)
\end{equation}
and $e_p=e_p(z)$ is the Stieltjes transform of a certain positive measure on $\mathbbm{R}^+$ with total mass $\tr(\bR)/p$, obtained as the unique solution in $\mathbbm{C}^+$ of the equation
\begin{equation}
  e_p = \frac{1}{p}\tr \left( \bR \left(\bA +  x_p(e_p) \bR - z\bI_p \right)^{-1} \right).
\end{equation}
\end{theorem}
This theorem is used repeatedly in papers \ref{sec:paper3} through \ref{sec:paper5}. It is that powerful and flexible. However, we often assume finite $4+\epsilon$ moment, while the theorem above assumes $8+\epsilon$. We can circumvent that by Theorem \citet{REF}... 

When we are able to construct sample estimators on the form of $\bB$ it does not necessarily imply that we can find analytic solutions. \textbf{comment more on it.}

\section{Shrinkage estimators in modern portfolio theory}
The estimator $\hV$ is clearly biased, it even diverges when $c$ approaches $1$. 
This problem is not unique. 
The least squares estimator is usually very volatile when there are many covariates in your regression model. 
An easy solution is to use $(1-c)\hV$ as an unbiased estimator for the variance of the GMV portfolio.
However, that might not be what we want. 
We want to create a good estimator for the weights, since these are what we invest in! 
There are many solutions to this problem but the most common is using a shrinkage estimator. 
We will introduce bias to our weights but hopefully reduce the variance.

Looking at the GMV portfolio, there are two natural extensions. Either, we regularize the sample covariance matrix $\bS$ or we regularize the weights $\hbw_{GMV}$ directly. Lets start with the latter. The first extension is to combine the GMV portfolio weights with some target portfolio $\bb$. We construct the shrunk portfolio weights $\hbw_{SH}$ as
\begin{equation}
  \hbw_{SH} = \alpha\hbw_{GMV} + (1-\alpha)\bb
\end{equation}
which introduces the bias $(1-\alpha)(\optn{E}[\hbw_{GMV}]+\bb) - \bw_{GMV}$ but decreases the variance to
\begin{align}
  \optn{E}\left[\left(\alpha\hbw_{GMV} - \alpha \optn{E}[\hbw_{GMV}]\right)\left(\alpha\hbw_{GMV} - \alpha \optn{E}[\hbw_{GMV}]\right)^\top\right] 
  & = 
  \alpha^2\optn{E}\left[\left(\hbw_{GMV} - \optn{E}[\hbw_{GMV}]\right)\left(\hbw_{GMV} - \optn{E}[\hbw_{GMV}]\right)^\top\right]
\end{align}
It now stands to determine $\alpha$. 
Shrinkage intensities are most often determined by cross validation (see e.g. \citet[ch. 5]{james2013introduction}). 
We try to find the best shrinkage coefficients by dividing data into a test and training set.
These are then used in conjunction with a Loss function to determine the optimal value for the shrinkage coefficients.
A natural choice of loss function for the GMV portfolio is the out-of-sample variance.
Our aim is to determine $\min_\alpha \hbw_{SH}^\top(\alpha) \bSigma \hbw_{SH}(\alpha)$.
The loss depends on $\bSigma$, which we do not know.
The perhaps most obvious solution is to use the test set to estimate the $\bSigma$. 
As we will see, that will have its own issues.
The second solution, employed by \citet{bodnar2018estimation}, is to first solve the optimization problem analytically.
Since the estimator is unobtainable, its usually referred to a \textit{oracle} estimator. 
To construct an estimator we can actually use, or what is called a \textit{bona-fide} estimator, we take the limit of the oracle estimator.
We then construct a consistent estimator for the limiting object at hand and use that one instead.

The two methods can differ quite substantially in their solution. 
One method is simple to implement while the other \textit{should} be theoretically superior.
In \ref{high-dim-CV} we display R-code for a small motivating example to why deriving bona-fide estimators can prove to be fruitful.
It is a comparison between the estimator from \citet{bodnar2018estimation} and determining the shrinkage coefficient using a 5 fold cross-validation procedure.
In this example we use $n=250, p=150$ and $\bb$ equal to the EW portfolio.
It is a large portfolio, though $c=p/n<1$.
%In this example we omit the code, but it is available on github\footnote{\url{https://github.com/Ethorsn/Phd-thesis}}.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\captionof{chunk}{R-code performing a 5-fold cross validation for determining shrinkage coefficients as well as the implementation from the HDShOP package.}\label{high-dim-CV}\begin{alltt}
\hlcom{# setup}
\hlkwd{set.seed}\hlstd{(}\hlnum{123}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlnum{150}
\hlstd{n} \hlkwb{<-} \hlnum{250}
\hlstd{K} \hlkwb{<-} \hlnum{5}
\hlstd{b} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,p)} \hlopt{/} \hlstd{p} \hlcom{# use EW portfolio as target }
\hlcom{# simulate params & dataset}
\hlstd{Sigma} \hlkwb{<-} \hlstd{HDShOP}\hlopt{::}\hlkwd{RandCovMtrx}\hlstd{(p)}
\hlstd{mu} \hlkwb{<-} \hlkwd{runif}\hlstd{(p,} \hlopt{-}\hlnum{0.1}\hlstd{,} \hlnum{0.1}\hlstd{)}
\hlstd{Y} \hlkwb{<-} \hlstd{mu} \hlopt{%*%} \hlkwd{t}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,n))} \hlopt{+} \hlkwd{t}\hlstd{(}\hlkwd{chol}\hlstd{(Sigma))} \hlopt{%*%} \hlkwd{matrix}\hlstd{(}\hlkwd{rt}\hlstd{(n}\hlopt{*}\hlstd{p,} \hlkwc{df}\hlstd{=}\hlnum{5}\hlstd{),} \hlkwc{ncol}\hlstd{=n)}
\hlcom{# create test splits}
\hlstd{folds} \hlkwb{<-} \hlkwd{split}\hlstd{(}\hlnum{1}\hlopt{:}\hlstd{n,} \hlnum{1}\hlopt{:}\hlstd{K)}
\hlstd{grid} \hlkwb{<-} \hlkwd{expand_grid}\hlstd{(}\hlstr{"alpha"} \hlstd{=} \hlkwd{seq}\hlstd{(}\hlnum{0.01}\hlstd{,} \hlnum{0.99}\hlstd{,} \hlkwc{by}\hlstd{=}\hlnum{0.01}\hlstd{),} \hlstr{"fold"} \hlstd{=} \hlnum{1}\hlopt{:}\hlstd{K)}
\hlcom{# perform 5-fold CV, pmap to map over rows }
\hlstd{result} \hlkwb{<-} \hlkwd{pmap}\hlstd{(grid,} \hlopt{~}\hlstd{\{}
    \hlstd{test} \hlkwb{<-} \hlstd{Y[,folds[[.y]]]}
    \hlstd{train} \hlkwb{<-} \hlstd{Y[,}\hlopt{-}\hlstd{folds[[.y]]]}
    \hlstd{S} \hlkwb{<-} \hlkwd{var}\hlstd{(}\hlkwd{t}\hlstd{(train))}
    \hlstd{S_inv} \hlkwb{<-} \hlkwd{solve}\hlstd{(S)}
    \hlstd{w} \hlkwb{<-} \hlstd{.x} \hlopt{*} \hlstd{S_inv} \hlopt{%*%} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{, p)} \hlopt{/} \hlkwd{sum}\hlstd{(S_inv)} \hlopt{+} \hlstd{(}\hlnum{1}\hlopt{-}\hlstd{.x)}\hlopt{*}\hlstd{b}
    \hlkwd{t}\hlstd{(w)} \hlopt{%*%} \hlkwd{var}\hlstd{(}\hlkwd{t}\hlstd{(test))} \hlopt{%*%} \hlstd{w}
  \hlstd{\})} \hlopt{%>%}
  \hlkwd{unlist}\hlstd{()} \hlopt{%>%}
  \hlkwd{tibble}\hlstd{(}\hlstr{"variance"}\hlstd{=.)} \hlopt{%>%}
  \hlkwd{bind_cols}\hlstd{(grid)} \hlopt{%>%}
  \hlkwd{group_by}\hlstd{(alpha)} \hlopt{%>%}
  \hlkwd{summarise}\hlstd{(}\hlkwc{loss} \hlstd{=} \hlkwd{mean}\hlstd{(variance),}
            \hlkwc{sd_loss} \hlstd{=} \hlkwd{var}\hlstd{(variance))}
\hlstd{min_vals} \hlkwb{<-} \hlkwd{filter}\hlstd{(result, loss} \hlopt{==} \hlkwd{min}\hlstd{(loss))}
\hlcom{# Use HDshop pkg to compute the weights}
\hlstd{w_bodnar2018} \hlkwb{<-} \hlstd{HDShOP}\hlopt{::}\hlkwd{MVShrinkPortfolio}\hlstd{(Y,} \hlkwc{gamma}\hlstd{=}\hlnum{Inf}\hlstd{,} \hlkwc{b}\hlstd{=b,} \hlkwc{beta}\hlstd{=}\hlnum{0.01}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{figure}

{\centering \includegraphics[width=\maxwidth]{figure/cv_benchmark-1} 

}

\caption[Out of sample variance estimates from the 5-fold cross validation]{Out of sample variance estimates from the 5-fold cross validation.}\label{fig:cv_benchmark}
\end{figure}

\end{knitrout}

In Figure \ref{fig:cv_benchmark} we illustrate the out of sample variance aggregated over folds. 
Cross validation suggest that the optimal value should be $0.68$. 
The analytic method from \citet{bodnar2018estimation} suggest that the optimal value is $0.7968$.
The "correct" value of $\alpha$, as given by the oracle estimator, is equal to $0.8151$. 
Its limiting value (see Theorem 2.1 \citet{bodnar2018estimation}) is equal to $0.7933$.
Although none of the methods are spot on, the bona-fide estimator is closer to what the optimal value should be.
Furthermore, changing the number of folds in the K-fold cross validation can give quite different results in what the optimal value should be.
This linear shrinkage approach is used in Paper \ref{sec:paper3} through \ref{sec:paper5}.

So far we have limited our approach to shrinking the weights. 
The next step is to shrink the elements of $\bS$.
If we do so then we can consider the case where $c>1$.
We leave this as an open question for the next section where we describe the papers in more detail.
%\begin{enumerate}
%	\item Other types of estimators and why they might be better than $\bS$.
%	\item Rotation-invariant estimation - what does it mean (we do not want to target eigenvectors)?
%\end{enumerate}

%%%%%% ------------------------------------------------------------------------
\chapter{Summary of papers}\label{ch:papersummary}
%%%%%% ------------------------------------------------------------------------


The papers presented here are among a total of ... papers produced. These are selected based their common theme.

\section{Paper 1 - Sampling Distributions of Optimal Portfolio Weights and Characteristics in Small and Large Dimensions}\label{sec:paper1}
The paper investigates a fundamental question in modern portfolio theory. 
What are the actual implications of using the sample covariance matrix $\bS$ and the sample mean $\bxb$ instead of the true covariance matrix $\bSigma$ and $\bmu$?
The paper does so when the returns follow a multivariate normal distribution. 
In it we derive the distribution for all optimal portfolios on the common form
\begin{equation}\label{eqn:paper1_eq1}
  \bL\hbw_{opt} = \bL\hbw_{GMV} + g(\hR, \hV, \hs)\bL\bv
\end{equation}
for some matrix $\bL$ of size $k \times p$ where $k<p$.
To do so, we derive the stochastic representation for the joint distribution of all quantities in the equation \eqref{eqn:paper1_eq1}. 
That enables us to efficiently simulate from the distribution, 
Using this representation we can easily compute quantiles of the joint distribution of the efficient frontier.
Furthermore, we also derive the high-dimensional asymptotic distribution of said joint distribution. 
The high-dimensional asymptotic distribution is then compared to different models.
One scenario considers simulations from the stochastic representation, trying to deduce the finite-sample properties.
The other scenarios try to investigate what happens when we deviate from the model.
As one can expect, the high-dimensional distribution works well under the assumptions though seem to be reasonably robust from deviations of the model.
The hardest part to determine is what different characteristics of the return distribution have on the sample distribution of the quantities in \eqref{eqn:paper1_eq1} it remains an unanswered question.

\section{Paper 2 -Tangency portfolio weights under a skew-normal model in small and large dimensions}\label{sec:paper2}
In this paper we investigate a portfolio which we mention in chapter \ref{ch:MPT}, the tangency portfolio. 
The portfolio is obtained from the quadratic utility function.
It originates from the following portfolio allocation problem
\begin{align}
  \min_{w_0,\bw} & w_0 r_f + \bw^\top \bmu - \frac{1}{2\gamma} \bw^\top \bSigma \bw \\
  \text{ s.t.} &\; w_0 + \bw^\top \ones_p = 1
\end{align}
Although the portfolio is most often called the tangency portfolio it actually quantifies all portfolios on the capital market line.
Increasing or decreasing the risk aversion coefficient $\gamma>0$ decrease or increase our position in the risky assets.
This paper extends Paper 1 as it considers investments in a risk-free asset and use an extension of the multivariate normal model, the CSN model presented in chapter \ref{ch:estim}. 
The model can include skewness in the asset returns, a trait they usually exhibit (see e.g. \citet{cont2001empirical}). 
Similarly to Paper 1, we derive the distribution of the sample tangency portfolio.
We investigate what implications the model has on the sample tangency portfolio.
In short, skewness results in a bias of the portfolio weights. 
The investor will not hold the correct portfolio on average.
Furthermore, we also investigate its high-dimensional distribution and show that it the skewness disappears asymptotically. 
The high-dimensional distribution is the same as the previous research have shown (see .e.g. \citet{karlsson2021statistical})

\section{Paper 3 - Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance Portfolio}\label{sec:paper3}
This paper deals with the fact that taking limits changes estimates. 
If the investor invest in a portfolio and then waits for a weak, month or year their perception of what portfolio they should hold will have changed. 
This is implicit when taking limits.
A natural question to ask is then how to go from one portfolio to another, e.g. how to rebalance optimally when you receive a new set of data. 
Assuming that the investor is ok with rebalancing the portfolio at fixed time points we develop a rebalancing scheme for the GMV portfolio.
This type of "dynamic" shrinkage has a lot of practical implications.
If you own one portfolio it most often cost money to go from that to the next.
That will take away from the return and profit you make.
Furthermore, its not always possible to go from one portfolio to the next on a day.
You can influence the market or make possibly make to large positional changes, which are not allowed.

\textbf{more comments...}
\section{Paper 4 - Is the empirical out-of-sample variance an informative risk measure for high-dimensional portfolios}\label{sec:paper4}
Any empirical application using the GMV portfolio is bound to include the volatility or variance. 
A natural question to ask is then; is the empirical out-of-sample variance a consistent estimator of the variance? 
Furthermore, is it a good option to use or are there perhaps better options of performance measures? 
In this paper we investigate another common metric of evaluation, the relative out-of-sample loss.   
We find that...

\textbf{more comments...}
\section{Paper 5 - Double shrinkage}\label{sec:paper5}
In this thesis we mostly use $\bS$ and cope with that the sample covariance matrix is a noisy estimator by linear shrinkage or understanding the "noise" it provides.
That implies that we have so far only consider $c<1$.
In this paper we use ... as well as the linear shrinkage from previous papers. 
That enables us to cover the case where $c>1$ and potentially create a more noise-insensitive estimator of the portfolio weights.
In this paper we consider the following shrinkage estimator
$$
\hbw_{SH} = \psi \frac{\left(\bS + \lambda \bI_p \right)^{-1}\ones_p}{\ones_p^\top\left(\bS + \lambda \bI_p \right)^{-1}\ones_p} + (1-\psi)\bb.
$$
Since we are dealing with the GMV portfolio we naturally use the out-of-sample loss as a target to determine $\psi$ and $\lambda$.
Using an estimate as this leaves all potential to recieving a closed form solution in terms of a oracle estimator.
What we get is a oracle loss function, which we can then derive a bona-fide estimator for.

The model is seen to perform on par with the non-linear shrinkage of \citet{lw20} in all simulations and beat their method in a empirical analysis.
Furthermore, it also increases/decreases the results of several other portfolio metrics.

\textbf{more comments...}
%\section*{Paper 6 - The capital market line, tangency portfolio and the effect of Tikhonov regularization in higher dimensions}\label{sec:paper6}

\section{Other research results}

Paper \ref{sec:paper3} is accompanied by a R package, available on CRAN. 
You are free, or rather encouraged(!), to install it with \hlkwd{install.packages}\hlstd{(}\hlstr{"DOSPortfolio"}\hlstd{)}. 
The package provides a simple interface for the methods implemented in the paper. 
In \ref{DOSportfolio} we display a short example on how to construct the dynamic portfolio weights. 
The package is the first iteration of possibly many more portfolios which can be constructed in a similar fashion.
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{1, 1, 1}\color{fgcolor}\begin{kframe}
\captionof{chunk}{R-code which showcase the use of the DOSPortfolio package.}\label{DOSportfolio}\begin{alltt}
\hlkwd{library}\hlstd{(DOSPortfolio)}
\hlstd{df} \hlkwb{<-} \hlkwd{read_csv}\hlstd{(}\hlstr{"../data/returns.csv"}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlnum{350}\hlstd{; n} \hlkwb{<-} \hlnum{400}
\hlcom{# Sample p assets}
\hlkwd{set.seed}\hlstd{(}\hlnum{1234}\hlstd{)}
\hlstd{asset_cols} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlnum{2}\hlopt{:}\hlkwd{ncol}\hlstd{(df),} \hlkwc{size} \hlstd{= p)}
\hlcom{# specify reallocation points}
\hlstd{reallocation_points} \hlkwb{<-} \hlkwd{seq}\hlstd{(n,} \hlkwd{nrow}\hlstd{(df),} \hlkwc{by}\hlstd{=n)}
\hlcom{# estimate portfolio weights}
\hlstd{dos_weights} \hlkwb{<-} \hlstd{df} \hlopt{%>%}
  \hlkwd{select}\hlstd{(}\hlkwd{all_of}\hlstd{(asset_cols),} \hlopt{-}\hlstd{date)} \hlopt{%>%}
  \hlkwd{DOSPortfolio}\hlstd{(.,}
               \hlkwc{reallocation_points} \hlstd{= reallocation_points,}
               \hlkwc{target_portfolio} \hlstd{=} \hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwd{ncol}\hlstd{(.))}\hlopt{/}\hlkwd{ncol}\hlstd{(.),}
               \hlkwc{shrinkage_type} \hlstd{=} \hlstr{"overlapping"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
Furthermore, the following papers were also coauthored throughout the writing of this thesis \cite{bodnar2020quantile}, \cite{bodnar2021bayesian}  and \cite{bodnar2021quantile}.
The first presents an analytic derivation of the MPT framework in the Bayesian setting. 
It specifically looks at how quantiles of optimal portfolios can be constructed and the effects of estimation uncertainty in these. 
This is especially important since the regulations in place demands that you report quantile-based risk measures (see \citet{basel4}).
The second paper provides a continuation on the first. 
Our idea is to model the investors beliefs explicitly and construct a prior which captures what the likelihood cant. 
We impose a prior distribution which adapts to the recent observations when the market is turbulent. 
The algorithm is seen to work well when markets are turbulent.
The third paper also considers quantile based portfolios. 
It does so in a general framework, not necessarily as the same framework as MPT where we only use the first two moments of the return distribution.



%%%%%% ------------------------------------------------------------------------
\chapter{Future research}\label{ch:future}
%%%%%% ------------------------------------------------------------------------


There are many possible extensions and future projects to the thesis at hand. We talk very much of estimation uncertainty and how to cope with it.
Bayesian statistics provide a straight forward way to integrate that. 
However, it demands indepth knowledge of MCMC and also good prior distributions.
Neither are easy tasks.
Another approach of incorporating estimation uncertainty is robust optimization.
Robust optimization, in a MPT setting, tries to incorporate the estimation uncertainty into the portfolio allocation problem itself.
The literature is large. 
Are there connections to be made and especially with Empirical Bayes?

In Paper \ref{sec:paper3} we consider the allocations points fixed.
That assumption can be limiting for some investors. 
Can we incorporate that decision process into the portfolio allocation problem?

BEKK models are usually hard to fit and use for MPT. 
Even when their coefficients have been estimated their forecasts are not always positive definite. 
The first issue can be solved if one can formulate the models as Recurrent Neural Networks and use deep-learning libraries Torch or Tensorflow to fit the models. 
These are tailored to solve the specifc problem of fitting very large models! Recent large Natural Languange Processing models have \textit{billions} of parameters. 
By placing BEKK models in this framework one also has the possibility to develop new models. 
The development is solely determined by constructing new layers to the networks. 
It would also be easier to integrate different sources of information in the models.

%Although not part of this thesis the "Flipped classroom" approach has been a big part the teaching procedure in the course MT4007. 
%In this course we use a lot of online learning tools. 
%The course has developed a bit through the years although it has stayed true to its original casting, trying to take teaching to scale. 
%It is very complicated to do well and is not straight forward what actually works, as it usually is with observational studies. 
%

\printbibliography
%\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Insert papers here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
